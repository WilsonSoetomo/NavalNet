Project Context: Deep RL Battleship Tournament1. Project OverviewObjective: Develop and compare two distinct Reinforcement Learning (RL) architectures—DQN and PPO—to play the game of Battleship.Goal: Identify the superior model and subsequently augment it with Monte Carlo Tree Search (MCTS) to achieve "AlphaZero-style" strategic depth.Environment: 10x10 grid. Standard ship sizes: 5, 4, 3, 3, 2.Timeline: 7-week academic quarter (Winter 2026, CS 175).2. Methodology & ComparisonWe are conducting a "Method-Driven" study by training two different types of agents:MethodTypeCharacteristicDQNValue-BasedLearns a Q-function (estimated reward for every square). Off-policy.PPOPolicy-BasedLearns a probability distribution over actions. On-policy.Training StagesBoth models will be trained iteratively against the following opponents:Random Agent: To establish basic motor skills (don't hit the same square twice).Heuristic Agent: To learn standard "Hunt and Seek" and "Checkerboard" strategies.Self-Play: The current model version plays against its previous iterations to discover optimal placement and shooting strategies simultaneously.3. Technical ArchitectureThe Agent (Multi-Head Output)Each model must handle two distinct phases of the game:Placement Head: Outputs 5 coordinates/orientations to set up the board.Shooting Head: Outputs a coordinate (0-99) for the strike.Observation Space: 10x10 matrix (0 = Unknown, 1 = Miss, 2 = Hit, 3 = Sunk).Phase 2: MCTS AugmentationThe "Winner" of the DQN vs. PPO tournament will be integrated with MCTS.NN Role: The NN acts as a Heuristic to prune the search tree.MCTS Role: Simulates "belief states" of where hidden ships might be and simulates outcomes to find the highest-probability strike.4. Evaluation Rubric RequirementsQuantitative: Average Turns to Win (ATTW), Win-Rate against Heuristics, Sample Complexity (Training speed), number of moves to sink a ship and number of moves between finding ships .Qualitative: Analysis of "Dirty Laundry" (failure modes), visualization of the MCTS "Thinking" heatmap, and an ablation study on the impact of "Sunk Ship" notifications.5. Coding Standards for LLMEnvironment: Python 3.10+, OpenAI Gym/Gymnasium API.Frameworks: PyTorch or Stable-Baselines3.Style: Modular components (Environment, Agent, MCTS). Ensure clear separation between the game engine and the RL logic.